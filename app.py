
import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import tempfile
import seaborn as sns
import matplotlib.pyplot as plt
import pytz
from datetime import datetime
from streamlit.components.v1 import html
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from function_preprocessing_motorbike import preprocess_motobike_data
from build_model_price_anomaly_detection import detect_outliers

st.set_page_config(page_title="Motorbike Price & Anomaly App", layout="wide")

# B·∫Øt ƒë·∫ßu ƒëo·∫°n code c·∫ßn th√™m ƒë·ªÉ √°p d·ª•ng justify (cƒÉn ƒë·ªÅu)
html_code = """
<style>
/* Ch·ªçn t·∫•t c·∫£ c√°c th√†nh ph·∫ßn ch·ª©a vƒÉn b·∫£n ch√≠nh c·ªßa Streamlit
   (nh∆∞ st.markdown, st.write, st.header, st.subheader, st.text, v.v.)
   v√† √°p d·ª•ng cƒÉn ƒë·ªÅu (text-align: justify;) */
.stMarkdown, .stText, .stHtml, .stHeader, .stSubheader, .stTitle, .stPageLink, .css-selector-cho-cac-phan-tu-khac {
    text-align: justify;
    text-justify: inter-word; /* D√†nh cho c√°c tr√¨nh duy·ªát IE/Edge */
}
/* M·ªôt s·ªë component nh∆∞ st.write/st.markdown s·∫Ω ƒë∆∞·ª£c b·ªçc trong class 'stMarkdown'
   v√† class n√†y c√≥ th·ªÉ ƒë∆∞·ª£c b·ªçc trong c√°c div kh√°c. Ta c·∫ßn selector m·∫°nh h∆°n. */
div.stMarkdown p, div.stMarkdown, div[data-testid="stText"] {
    text-align: justify;
    text-justify: inter-word;
}
</style>
"""
st.components.v1.html(html_code, height=0)

# 1. B·∫ÆT ƒê·∫¶U: CSS ƒê·ªÇ LO·∫†I B·ªé GI·ªöI H·∫†N CHI·ªÄU R·ªòNG T·ªêI ƒêA C·ª¶A STREAMLIT
# ƒê·∫∑t max-width r·∫•t l·ªõn (v√≠ d·ª•: 2000px) ƒë·ªÉ cho ph√©p h√¨nh ·∫£nh hi·ªÉn th·ªã r·ªông h∆°n
html_code_width = """
<style>
/* Lo·∫°i b·ªè gi·ªõi h·∫°n max-width c·ªßa kh·ªëi n·ªôi dung ch√≠nh */
.main .block-container {
    max-width: 2000px !important; 
    padding-left: 1rem;
    padding-right: 1rem;
}
</style>
"""
st.components.v1.html(html_code_width, height=0)
# K·∫æT TH√öC: CSS ƒê·ªÇ LO·∫†I B·ªé GI·ªöI H·∫†N CHI·ªÄU R·ªòNG T·ªêI ƒêA

MODEL_PATH = "motobike_price_prediction_model.pkl"
TRAINING_DATA = "data_motobikes.xlsx"  # optional, used to compute brand_meanprice & grouping to match train

@st.cache_resource
def load_model(path=MODEL_PATH):
    if not os.path.exists(path):
        return None
    with open(path, "rb") as f:
        return pickle.load(f)

@st.cache_data
def build_training_helpers(path=TRAINING_DATA):
    """
    Load training data & build grouping rules + statistical thresholds
    (p10/p90, residual mean/std) for anomaly detection.
    """
    if not os.path.exists(path):
        return None

    try:
        df_train = preprocess_motobike_data(path)
        # =============== LOAD MODELS ===============
        with open("unsup_scaler.pkl", "rb") as f:
            scaler = pickle.load(f)

        with open("kmeans_model.pkl", "rb") as f:
            kmeans = pickle.load(f)

        # =============== 1) BRAND GROUPING ==================
        brand_counts = df_train['brand'].value_counts()
        rare_brands = set(brand_counts[brand_counts < 50].index)

        # model grouping by brand_grouped
        model_group_maps = {}
        for bg, g in df_train.groupby('brand_grouped'):
            counts = g['model'].value_counts()
            rare_models = set(counts[counts < 100].index)
            model_group_maps[bg] = rare_models

        # mean price for brand
        brand_mean_map = df_train.groupby('brand')['brand_meanprice'].first().to_dict()

        # =============== 2) PRICE P10/P90 BY SEGMENT ==================
        seg_price_stats = (
            df_train.groupby('segment')['price']
                    .quantile([0.10, 0.90])
                    .unstack(level=1)
                    .rename(columns={0.10:'p10', 0.90:'p90'})
        ).reset_index()

        seg_price_map = seg_price_stats.set_index('segment').to_dict('index')
        # format: seg_price_map[segment] = {'p10':..., 'p90':...}

        # =============== 3) RESIDUAL STATS BY SEGMENT ==================

        # Load model
        with open(MODEL_PATH, 'rb') as f:
            model = pickle.load(f)

        # Define cols
        cat_cols = ['segment','bike_type','origin','engine_capacity']
        num_cols = ['age','mileage_km','min_price','max_price','brand_meanprice']

        # Build matrix
        X = df_train[cat_cols + num_cols]
        # y = df['log_price']

        # Predict price
        df_train['price_hat'] = np.expm1(model.predict(X))
        df_train['resid'] = df_train['price'] - df_train['price_hat']  # price_hat t·ª´ preprocess

        seg_resid_stats = (
            df_train.groupby('segment')['resid']
                    .agg(['mean', 'std'])
                    .rename(columns={'mean': 'resid_mean', 'std': 'resid_std'})
        ).reset_index()

        seg_resid_map = seg_resid_stats.set_index('segment').to_dict('index')
        # format: seg_resid_map[seg] = {'resid_mean':..., 'resid_std':...}

        return {
            'rare_brands': rare_brands,
            'model_group_maps': model_group_maps,
            'brand_mean_map': brand_mean_map,
            'seg_price_map': seg_price_map,
            'seg_resid_map': seg_resid_map
        }

    except Exception as e:
        print("Error building helpers:", e)
        return None


helpers = build_training_helpers(TRAINING_DATA)
model = load_model(MODEL_PATH)

st.title("Motorbike Price Prediction & Anomaly Detection")
# st.markdown("·ª®ng d·ª•ng cho ph√©p: 1) D·ª± ƒëo√°n gi√° xe m√°y (nh·∫≠p tay ho·∫∑c upload file) 2) Ph√°t hi·ªán xe b·∫•t th∆∞·ªùng (upload file)")
# st.image("xe_may_cu.jpg", caption="Xe m√°y c≈©")
st.image("xe_may_cu.jpg", caption="Xe m√°y c≈©", width=1000)

# page = st.sidebar.selectbox("Ch·ªçn ch·ª©c nƒÉng", ["D·ª± ƒëo√°n gi√°", "Ph√°t hi·ªán b·∫•t th∆∞·ªùng"])
menu = ["Gi·ªõi thi·ªáu", "B√†i to√°n nghi·ªáp v·ª•", "ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o", "D·ª± ƒëo√°n gi√°", "Ph√°t hi·ªán xe b·∫•t th∆∞·ªùng"]
page = st.sidebar.selectbox('Menu', menu)


@st.cache_data
def load_reference_data():
    return preprocess_motobike_data(TRAINING_DATA)

df_ref = load_reference_data()
brand_list = sorted(df_ref['brand_grouped'].dropna().unique())
model_list = sorted(df_ref['model_grouped'].dropna().unique())
bike_type_list = sorted(df_ref['bike_type'].dropna().unique())
origin_list = sorted(df_ref['origin'].dropna().unique())
engine_capacity_list = sorted(df_ref['engine_capacity'].dropna().unique())

if page == 'Gi·ªõi thi·ªáu':

    st.subheader("[Trang ch·ªß](https://www.chotot.com/)")
    
    st.header('Gi·ªõi thi·ªáu d·ª± √°n')
    st.markdown('''ƒê√¢y l√† d·ª± √°n x√¢y d·ª±ng h·ªá th·ªëng h·ªó tr·ª£ **ƒë·ªãnh gi√° xe m√°y c≈©** v√† **ph√°t hi·ªán tin ƒëƒÉng b·∫•t th∆∞·ªùng** tr√™n n·ªÅn t·∫£ng *Ch·ª£ T·ªët* - trong kh√≥a ƒë·ªì √°n t·ªët nghi·ªáp Data Science and Machine Learning 2024 l·ªõp DL07_K308 c·ªßa nh√≥m 6. \nTh√†nh vi√™n nh√≥m g·ªìm c√≥:
        \n1. V≈© Th·ªã Ng·ªçc Anh \n2. Nguy·ªÖn Ph·∫°m Qu·ª≥nh Anh''')
    
    st.header('M·ª•c ti√™u c·ªßa d·ª± √°n')
    # st.text('''1. T·∫°o m√¥ h√¨nh ƒë·ªÅ xu·∫•t xe m√°y t∆∞∆°ng t·ª± ƒë·ªëi v·ªõi m·∫´u xe ƒë∆∞·ª£c ch·ªçn ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm do ng∆∞·ªùi d√πng cung c·∫•p.\n2. Ph√¢n kh√∫c th·ªã tr∆∞·ªùng xe m√°y''')
    st.write("""
    M·ª•c ti√™u c·ªßa d·ª± √°n:
    - **TƒÉng c∆∞·ªùng minh b·∫°ch** th·ªã tr∆∞·ªùng xe m√°y c≈© th√¥ng qua d·ª± b√°o gi√° h·ª£p l√Ω.
    - **Ph√°t hi·ªán c√°c tin ƒëƒÉng b·∫•t th∆∞·ªùng**, gi√∫p l·ªçc ra xe c√≥ gi√° ho·∫∑c th√¥ng tin sai l·ªách.
    - **H·ªó tr·ª£ ng∆∞·ªùi d√πng** ƒë∆∞a ra quy·∫øt ƒë·ªãnh mua/b√°n ch√≠nh x√°c v√† tin c·∫≠y h∆°n.
    - **T·ªëi ∆∞u h√≥a quy tr√¨nh ki·ªÉm duy·ªát** th√¥ng tin tr√™n n·ªÅn t·∫£ng giao d·ªãch xe m√°y.
    """)

    st.header('Ph√¢n c√¥ng c√¥ng vi·ªác')

    st.write("""
        - X·ª≠ l√Ω d·ªØ li·ªáu: Ng·ªçc Anh v√† Qu·ª≥nh Anh
        - D·ª± ƒëo√°n gi√° xe theo ph∆∞∆°ng ph√°p ML truy·ªÅn th·ªëng: Ng·ªçc Anh v√† Qu·ª≥nh Anh
        - D·ª± ƒëo√°n gi√° xe theo PySpark: Ng·ªçc Anh
        - Ph√°t hi·ªán gi√° b·∫•t th∆∞·ªùng: Ng·ªçc Anh
        - L√†m slide: Ng·ªçc Anh v√† Qu·ª≥nh Anh
        - Giao di·ªán streamlit: Ng·ªçc Anh

        """)
elif page == 'B√†i to√°n nghi·ªáp v·ª•':
    st.subheader("[Trang ch·ªß](https://www.chotot.com/)")

    st.markdown("""

        ### V·∫•n ƒë·ªÅ nghi·ªáp v·ª•
        - Gi√° ni√™m y·∫øt kh√¥ng ƒë·ªìng nh·∫•t, kh√≥ x√°c ƒë·ªãnh gi√° th·ªã tr∆∞·ªùng.
        - Nhi·ªÅu tin ƒëƒÉng c√≥ gi√° b·∫•t th∆∞·ªùng g√¢y nhi·ªÖu d·ªØ li·ªáu.
        - Ki·ªÉm duy·ªát th·ªß c√¥ng t·ªën th·ªùi gian v√† kh√¥ng nh·∫•t qu√°n.
        - C·∫ßn m·ªôt h·ªá th·ªëng d·ª± b√°o gi√° v√† c·∫£nh b√°o b·∫•t th∆∞·ªùng t·ª± ƒë·ªông.

        ---

        ### B√†i to√°n ƒë·∫∑t ra
        - X√¢y d·ª±ng m√¥ h√¨nh **Price Prediction**.
        - Thi·∫øt k·∫ø m√¥ h√¨nh **Anomaly Detection** (ML-based + Rule-based).
        - T·ªëi ∆∞u quy tr√¨nh ki·ªÉm duy·ªát v√† n√¢ng cao ch·∫•t l∆∞·ª£ng tin ƒëƒÉng.

        ---

        ### Ph·∫°m vi tri·ªÉn khai
        - T·ªëi ∆∞u & chu·∫©n h√≥a d·ªØ li·ªáu th√¥.
        - T·∫°o ƒë·∫∑c tr∆∞ng cho m√¥ h√¨nh d·ª± ƒëo√°n gi√°.
        - Hu·∫•n luy·ªán m√¥ h√¨nh **Regression** ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng gi√° th·ªã tr∆∞·ªùng.
        - X√¢y d·ª±ng h·ªá th·ªëng g·∫Øn c·ªù b·∫•t th∆∞·ªùng g·ªìm:
        - **Model-based score** (ML)
        - **Business rule score** (Rule-based)
        - Tri·ªÉn khai giao di·ªán demo b·∫±ng **Streamlit**.

    """)

    # st.header("###Thu th·∫≠p d·ªØ li·ªáu")

    st.markdown("""  
    ### Thu th·∫≠p d·ªØ li·ªáu    
    B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).  

    B·ªô d·ªØ li·ªáu bao g·ªìm c√°c th√¥ng tin sau:

    - **id**: s·ªë th·ª© t·ª± c·ªßa s·∫£n ph·∫©m trong b·ªô d·ªØ li·ªáu  
    - **Ti√™u ƒë·ªÅ**: t·ª±a ƒë·ªÅ b√†i ƒëƒÉng b√°n s·∫£n ph·∫©m  
    - **Gi√°**: gi√° b√°n c·ªßa xe m√°y  
    - **Kho·∫£ng gi√° min**: gi√° s√†n ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
    - **Kho·∫£ng gi√° max**: gi√° tr·∫ßn ∆∞·ªõc t√≠nh c·ªßa xe m√°y  
    - **ƒê·ªãa ch·ªâ**: ƒë·ªãa ch·ªâ giao d·ªãch (ph∆∞·ªùng, qu·∫≠n, th√†nh ph·ªë H·ªì Ch√≠ Minh)  
    - **M√¥ t·∫£ chi ti·∫øt**: m√¥ t·∫£ th√™m v·ªÅ s·∫£n ph·∫©m ‚Äî ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t, t√¨nh tr·∫°ng, th√¥ng tin kh√°c  
    - **Th∆∞∆°ng hi·ªáu**: h√£ng s·∫£n xu·∫•t (Honda, Yamaha, Piaggio, SYM‚Ä¶)  
    - **D√≤ng xe**: d√≤ng xe c·ª• th·ªÉ (Air Blade, Vespa, Exciter, LEAD, Vario, ‚Ä¶)  
    - **NƒÉm ƒëƒÉng k√Ω**: nƒÉm ƒëƒÉng k√Ω l·∫ßn ƒë·∫ßu c·ªßa xe  
    - **S·ªë km ƒë√£ ƒëi**: s·ªë kilomet xe ƒë√£ v·∫≠n h√†nh  
    - **T√¨nh tr·∫°ng**: t√¨nh tr·∫°ng hi·ªán t·∫°i (v√≠ d·ª•: ƒë√£ s·ª≠ d·ª•ng)  
    - **Lo·∫°i xe**: Xe s·ªë, Tay ga, Tay c√¥n/Moto  
    - **Dung t√≠ch xe**: dung t√≠ch xi-lanh (v√≠ d·ª•: D∆∞·ªõi 50cc, 50‚Äì100cc, 100‚Äì175cc, ‚Ä¶)  
    - **Xu·∫•t x·ª©**: qu·ªëc gia s·∫£n xu·∫•t (Vi·ªát Nam, ƒê√†i Loan, Nh·∫≠t B·∫£n, ...)  
    - **Ch√≠nh s√°ch b·∫£o h√†nh**: th√¥ng tin b·∫£o h√†nh n·∫øu c√≥  
    - **Tr·ªçng l∆∞·ª£ng**: tr·ªçng l∆∞·ª£ng ∆∞·ªõc t√≠nh c·ªßa xe  
    - **Href**: ƒë∆∞·ªùng d·∫´n t·ªõi b√†i ƒëƒÉng s·∫£n ph·∫©m  
    """)


elif page == 'ƒê√°nh gi√° m√¥ h√¨nh v√† B√°o c√°o':    
    st.subheader("[Trang ch·ªß](https://www.chotot.com/)")  

    # df_home = preprocess_motobike_data(TRAINING_DATA)
    # st.subheader("D·ªØ li·ªáu xe m√°y c≈© (10 m·∫´u)")
    # st.dataframe(df_home.head(10))
    # st.subheader("Quy tr√¨nh th·ª±c hi·ªán")
    st.subheader("I. Th·ªëng k√™ m√¥ t·∫£ s∆° b·ªô")

    # st.markdown("""
    # **1. Th·ªëng k√™ m√¥ t·∫£ s∆° b·ªô** 
    # """)
    st.markdown("""        
    B·ªô d·ªØ li·ªáu g·ªìm **7.208 tin ƒëƒÉng** v·ªõi **18 thu·ªôc t√≠nh** (th∆∞∆°ng hi·ªáu, d√≤ng xe, s·ªë km, nƒÉm ƒëƒÉng k√Ω, gi√° ni√™m y·∫øt, m√¥ t·∫£‚Ä¶) ƒë∆∞·ª£c thu th·∫≠p t·ª´ n·ªÅn t·∫£ng **Ch·ª£ T·ªët** (tr∆∞·ªõc ng√†y 01/07/2025).  
                """)
    # --- V·∫Ω bi·ªÉu ƒë·ªì ---

    # # Hi·ªÉn th·ªã 4 bi·ªÉu ƒë·ªì d·∫°ng l∆∞·ªõi 2x2
    # col1, col2 = st.columns(2)
    # with col1:
    #     st.image("brand_grouped_count.png")
    #     st.image("age_bin_stats.png")

    # with col2:
    #     st.image("price_bin_stats.png")
    #     st.image("mileage_bin_stats.png")

    # ƒê·∫∑t chi·ªÅu r·ªông cho t·ª´ng h√¨nh ·∫£nh l√† 500px
    # T·ªïng chi·ªÅu r·ªông 2 c·ªôt s·∫Ω l√† 1000px
    image_width = 500
    
    # Hi·ªÉn th·ªã 4 bi·ªÉu ƒë·ªì d·∫°ng l∆∞·ªõi 2x2
    col1, col2 = st.columns(2)
    with col1:
        st.image("brand_grouped_count.png", width=image_width) # Th√™m width=500
        st.image("age_bin_stats.png", width=image_width)       # Th√™m width=500

    with col2:
        st.image("price_bin_stats.png", width=image_width)     # Th√™m width=500
        st.image("mileage_bin_stats.png", width=image_width)   # Th√™m width=500

    st.subheader("II. M√¥ h√¨nh d·ª± ƒëo√°n gi√° xe m√°y")

    st.markdown("""

    ##### L·ª±a ch·ªçn thu·ªôc t√≠nh           
    ƒê·ªÉ x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n gi√° xe m√°y, ch√∫ng t√¥i ƒë√£ ch·ªçn l·ªçc c√°c thu·ªôc t√≠nh ƒë·∫ßu v√†o (input features) c√≥ t√≠nh ch·∫•t d·ª± b√°o cao, bao g·ªìm: **Th∆∞∆°ng hi·ªáu, D√≤ng xe, Tu·ªïi xe, S·ªë km ƒë√£ ƒëi, Lo·∫°i xe, Dung t√≠ch xe, Xu·∫•t x·ª©, Kho·∫£ng gi√° min,** v√† **Kho·∫£ng gi√° max**.
                         
    ##### ƒê√°nh gi√° m√¥ h√¨nh
                
    Ch√∫ng t√¥i th·ª≠ nghi·ªám nhi·ªÅu m√¥ h√¨nh machine learning, bao g·ªìm **Random Forest, SVR, Gradient Boosting, Decision Tree** v√† **Linear Regression**. Trong s·ªë ƒë√≥, **Random Forest** cho k·∫øt qu·∫£ v∆∞·ª£t tr·ªôi nh·∫•t, th·ªÉ hi·ªán r√µ qua b·∫£ng d∆∞·ªõi ƒë√¢y:
    ##### üìä So s√°nh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh

    | M√¥ h√¨nh              | R¬≤       | MAE (VNƒê)        | RMSE (VNƒê)       |
    |---------------------|----------|------------------|------------------|
    | **Random Forest**    | 0.888230 | 4,381,802        | 7,635,801        |
    | **SVR**              | 0.871969 | 4,607,752        | 8,172,413        |
    | **Gradient Boosting**| 0.851320 | 4,884,985        | 8,806,793        |
    | **Decision Tree**    | 0.813617 | 5,319,813        | 9,860,408        |
    | **Linear Regression**| 0.731268 | 6,343,373        | 11,840,010       |
    
    """)
    st.image("actual_vs_predicted.png")
    st.markdown("""
    => K·∫øt qu·∫£ ƒë√°nh gi√° cho th·∫•y **Random Forest l√† m√¥ h√¨nh c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t**. Do ƒë√≥, ch√∫ng t√¥i l·ª±a ch·ªçn Random Forest l√†m m√¥ h√¨nh ch√≠nh cho b√†i to√°n **d·ª± ƒëo√°n gi√° xe m√°y**.
                """)
    
    st.subheader("III. M√¥ h√¨nh ph√°t hi·ªán xe b·∫•t th∆∞·ªùng")

    # st.markdown("""
    #     ###### H·ªá th·ªëng ph√°t hi·ªán b·∫•t th∆∞·ªùng ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n **hai nh√≥m ti√™u ch√≠**: **ƒêi·ªÉm s·ªë t·ª´ m√¥ h√¨nh h·ªçc m√°y** (`score_model_based`) v√† ƒêi·ªÉm s·ªë t·ª´ logic nghi·ªáp v·ª•** (`score_business_based`) 
    #         """)
    # st.markdown("""
    #     ###### Hai nh√≥m ti√™u ch√≠ n√†y ƒë∆∞·ª£c k·∫øt h·ª£p nh·∫±m ƒë·∫£m b·∫£o vi·ªác ph√°t hi·ªán b·∫•t th∆∞·ªùng v·ª´a **kh√°ch quan theo m√¥ h√¨nh**, v·ª´a **ph√π h·ª£p th·ª±c t·∫ø kinh doanh**.   
    #             """)
    st.markdown("""
        ###### H·ªá th·ªëng ph√°t hi·ªán b·∫•t th∆∞·ªùng ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n **hai nh√≥m ti√™u ch√≠**:

        * **ƒêi·ªÉm s·ªë t·ª´ m√¥ h√¨nh h·ªçc m√°y** (`score_model_based`): ƒê·∫£m b·∫£o vi·ªác ph√°t hi·ªán b·∫•t th∆∞·ªùng mang t√≠nh **kh√°ch quan theo m√¥ h√¨nh**.
        * **ƒêi·ªÉm s·ªë t·ª´ logic nghi·ªáp v·ª•** (`score_business_based`): ƒê·∫£m b·∫£o vi·ªác ph√°t hi·ªán b·∫•t th∆∞·ªùng **ph√π h·ª£p th·ª±c t·∫ø kinh doanh**.

        Hai nh√≥m ti√™u ch√≠ n√†y ƒë∆∞·ª£c k·∫øt h·ª£p nh·∫±m mang l·∫°i k·∫øt qu·∫£ ph√°t hi·ªán b·∫•t th∆∞·ªùng to√†n di·ªán v√† ƒë√°ng tin c·∫≠y.
        """)

    st.markdown("""
        #### 1. Ti√™u ch√≠ ƒë√°nh d·∫•u b·∫•t th∆∞·ªùng theo Logic H·ªçc m√°y (`score_model_based`)

        H·ªá th·ªëng s·ª≠ d·ª•ng **b·ªën ti√™u ch√≠** ch√≠nh d·ª±a tr√™n m√¥ h√¨nh th·ªëng k√™ v√† h·ªçc m√°y ƒë·ªÉ g√°n ƒëi·ªÉm b·∫•t th∆∞·ªùng:

        ---

        ##### 1.1. **`flag_resid` ‚Äì D·ª±a tr√™n ph·∫ßn d∆∞ (Residual Z-score)**
        * **Ng∆∞·ª°ng**: ƒê∆∞·ª£c ƒë·∫∑t l√† **3**.
        * **ƒê√°nh d·∫•u b·∫•t th∆∞·ªùng**: N·∫øu **Residual Z-score > 3**, `flag_resid = 1`.
        * **B√¨nh th∆∞·ªùng**: N·∫øu kh√¥ng, `flag_resid = 0`.

        ---

        ##### 1.2. **`flag_minmax` ‚Äì D·ª±a tr√™n kho·∫£ng gi√° h·ª£p l√Ω**
        * **ƒê√°nh d·∫•u b·∫•t th∆∞·ªùng**: N·∫øu **gi√° ni√™m y·∫øt** n·∫±m **ngo√†i kho·∫£ng gi√° Min-Max** ƒë∆∞·ª£c khai b√°o, `flag_minmax = 1`.
        * **B√¨nh th∆∞·ªùng**: N·∫øu kh√¥ng, `flag_minmax = 0`.

        ---

        ##### 1.3. **`flag_p10p90` ‚Äì D·ª±a tr√™n Ph√¢n v·ªã theo Ph√¢n kh√∫c**
        * **C∆° s·ªü**: X√°c ƒë·ªãnh **Ph√¢n v·ªã 10% (P10)** v√† **90% (P90)** c·ªßa gi√° xe trong t·ª´ng ph√¢n kh√∫c.
        * **ƒê√°nh d·∫•u b·∫•t th∆∞·ªùng**: N·∫øu gi√° tr·ªã n·∫±m **ngo√†i kho·∫£ng P10‚ÄìP90**, `flag_p10p90 = 1`.
        * **B√¨nh th∆∞·ªùng**: N·∫øu kh√¥ng, `flag_p10p90 = 0`.

        ---

        ##### 1.4. **`flag_unsup` ‚Äì T·ªïng h·ª£p t·ª´ H·ªçc m√°y kh√¥ng gi√°m s√°t**
        * **M√¥ h√¨nh**: K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ ba m√¥ h√¨nh ch√≠nh: **Isolation Forest, Local Outlier Factor, v√† KMeans**.
        * **Ti√™u ch√≠ KMeans**: ƒêi·ªÉm b·∫•t th∆∞·ªùng c√≥ s·ªë ƒëi·ªÉm trong c·ª•m nh·ªè h∆°n 10% t·ªïng th·ªÉ ho·∫∑c n·∫±m trong 5% ƒëi·ªÉm xa t√¢m c·ª•m nh·∫•t.
        * **ƒê√°nh d·∫•u b·∫•t th∆∞·ªùng**: N·∫øu **hai trong ba** m√¥ h√¨nh tr√™n ƒë√°nh d·∫•u b·∫•t th∆∞·ªùng, `flag_unsup = 1`.

        ---

        ##### üìà T√≠nh to√°n `score_model_based`
        ƒêi·ªÉm logic theo m√¥ h√¨nh (`score_model_based`) l√† t·ªïng c√≥ tr·ªçng s·ªë c·ªßa 4 ti√™u ch√≠ tr√™n, trong ƒë√≥ **`flag_resid`** c√≥ **tr·ªçng s·ªë 0.4**, v√† c√°c ti√™u ch√≠ c√≤n l·∫°i c√≥ tr·ªçng s·ªë **0.2**.

        ---

        #### 2. Ti√™u ch√≠ ƒë√°nh d·∫•u b·∫•t th∆∞·ªùng theo Logic Nghi·ªáp v·ª• (`score_business_based`)

        Ti√™u ch√≠ n√†y t·∫≠p trung v√†o s·ª± b·∫•t th∆∞·ªùng c·ªßa m·ªëi quan h·ªá gi·ªØa **S·ªë km ƒë√£ ƒëi** v√† **Tu·ªïi xe**:

        * **Nghi v·∫•n Tua c√¥ng-t∆°-m√©t (Qu√° th·∫•p)**: N·∫øu **S·ªë km ƒë√£ ƒëi < 200 * Tu·ªïi xe**.
        * **S·ªë km cao b·∫•t th∆∞·ªùng (Khai th√°c/Khai b√°o sai)**: N·∫øu **S·ªë km ƒë√£ ƒëi > 20000 * Tu·ªïi xe**.

        ---

        #### 3. T·ªïng h·ª£p v√† ƒê√°nh d·∫•u cu·ªëi c√πng

        * **ƒêi·ªÉm t·ªïng h·ª£p cu·ªëi c√πng (`final_score`)** l√† t·ªïng c·ªßa hai ƒëi·ªÉm: **`score_model_based`** v√† **`score_business_based`**.
        * **ƒê√°nh d·∫•u B·∫•t th∆∞·ªùng**: Xe c√≥ t·ªïng ƒëi·ªÉm **l·ªõn h∆°n 50** s·∫Ω ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† **B·∫•t th∆∞·ªùng**.
        """)

    st.markdown("##### V√≠ d·ª• 10 m·∫´u xe b·∫•t th∆∞·ªùng ƒë∆∞·ª£c ph√°t hi·ªán:")
    df_anomaly = pd.read_csv("outliers_detected_full.csv")
    st.dataframe(df_anomaly.sort_values('final_score', ascending=False).head(10))
    
elif page == "D·ª± ƒëo√°n gi√°":

    # --- PREDICTION PAGE ---

    st.header("D·ª± ƒëo√°n gi√° xe m√°y")

    mode = st.radio("Ch·ªçn c√°ch input:", ["Nh·∫≠p tay m·ªôt xe", "Upload file (Excel/CSV) ƒë·ªÉ d·ª± ƒëo√°n nhi·ªÅu xe)"])

    if mode == "Nh·∫≠p tay m·ªôt xe":
        col1, col2 = st.columns(2)
        with col1:
            brand = st.selectbox("Th∆∞∆°ng hi·ªáu (brand)", options=brand_list)
            model_name = st.selectbox("D√≤ng xe (model)", options=model_list)
            bike_type = st.selectbox("Lo·∫°i xe (bike_type)", options=bike_type_list)
            origin = st.selectbox("Xu·∫•t x·ª© (origin)", options=origin_list)
        with col2:
            engine_capacity = st.selectbox("Dung t√≠ch (engine_capacity)", options=engine_capacity_list)
            registration_year = st.number_input("NƒÉm ƒëƒÉng k√Ω", min_value=1980, max_value=2025, value=2019)
            mileage_km = st.number_input("S·ªë km ƒë√£ ƒëi", min_value=0, value=10000)
            min_price = st.number_input("Kho·∫£ng gi√° min (VND)", min_value=0, value=0)
            max_price = st.number_input("Kho·∫£ng gi√° max (VND)", min_value=0, value=0)

        if st.button("Ch·∫°y d·ª± ƒëo√°n"):
            if model is None:
                st.error(f"Kh√¥ng t√¨m th·∫•y model t·∫°i '{MODEL_PATH}'. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i.")
            else:
                # create df
                df_in = pd.DataFrame([{ 
                    'brand': brand,
                    'model': model_name,
                    'bike_type': bike_type,
                    'origin': origin,
                    'engine_capacity': engine_capacity,
                    'registration_year': registration_year,
                    'mileage_km': mileage_km,
                    'min_price': min_price if min_price>0 else np.nan,
                    'max_price': max_price if max_price>0 else np.nan
                }])

                # compute age
                current_year = 2025
                df_in['age'] = current_year - pd.to_numeric(df_in['registration_year'], errors='coerce')

                # apply grouping using helpers if available
                if helpers is not None:
                    # brand_grouped
                    if df_in.at[0, 'brand'] in helpers['rare_brands']:
                        df_in['brand_grouped'] = 'H√£ng kh√°c'
                    else:
                        df_in['brand_grouped'] = df_in['brand']

                    # model_grouped
                    bg = df_in.at[0, 'brand_grouped']
                    rare_models = helpers['model_group_maps'].get(bg, set())
                    if df_in.at[0, 'model'] in rare_models:
                        df_in['model_grouped'] = 'D√≤ng kh√°c'
                    else:
                        df_in['model_grouped'] = df_in['model']

                    # segment
                    df_in['segment'] = df_in['brand_grouped'] + '_' + df_in['model_grouped']

                    # brand_meanprice
                    df_in['brand_meanprice'] = helpers['brand_mean_map'].get(df_in.at[0,'brand'], np.nan)
                else:
                    # fallback simple
                    df_in['brand_grouped'] = df_in['brand']
                    df_in['model_grouped'] = df_in['model']
                    df_in['segment'] = df_in['brand'] + '_' + df_in['model']
                    df_in['brand_meanprice'] = np.nan
                    st.warning("Kh√¥ng t√¨m th·∫•y data hu·∫•n luy·ªán (data_motobikes.xlsx). App s·∫Ω d√πng fallback ‚Äî brand_meanprice c√≥ th·ªÉ l√† NaN, d·ª± ƒëo√°n c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c.")

                # ensure columns order and types as model training
                cat_cols = ['segment','bike_type','origin','engine_capacity']
                num_cols = ['age','mileage_km','min_price','max_price','brand_meanprice']
                X_pred = df_in[cat_cols + num_cols]

                # predict
                try:
                    log_hat = model.predict(X_pred)
                    price_hat = np.expm1(log_hat)
                    df_in['predicted_price'] = price_hat

                    st.success(f"Gi√° d·ª± ƒëo√°n: {int(price_hat[0]):,} VND")
                    st.dataframe(df_in[['brand','model','bike_type','origin','engine_capacity','predicted_price']]) # ch·ªânh in df ·ªü ƒë√¢y

                except Exception as e:
                    st.exception(e)

    else: 
        st.subheader("Upload file ƒë·ªÉ d·ª± ƒëo√°n nhi·ªÅu xe (Excel/CSV)")
        uploaded_file = st.file_uploader("Ch·ªçn file (xlsx/csv)", type=['xlsx','csv'])

        if uploaded_file is not None:
            # save to temp
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name

            try:
                # ===============================
                # 1) Load file raw
                # ===============================
                if uploaded_file.name.endswith(".csv"):
                    df_raw = pd.read_csv(tmp_path)
                else:
                    df_raw = pd.read_excel(tmp_path)
                    df_raw = df_raw.rename(columns={
                        'Gi√°': 'price',
                        'Kho·∫£ng gi√° min': 'min_price',
                        'Kho·∫£ng gi√° max': 'max_price',
                        'Th∆∞∆°ng hi·ªáu': 'brand',
                        'D√≤ng xe': 'model',
                        'NƒÉm ƒëƒÉng k√Ω': 'registration_year',
                        'S·ªë Km ƒë√£ ƒëi': 'mileage_km',
                        'T√¨nh tr·∫°ng': 'condition',
                        'Lo·∫°i xe': 'bike_type',
                        'Dung t√≠ch xe': 'engine_capacity',
                        'Xu·∫•t x·ª©': 'origin',
                        'Ch√≠nh s√°ch b·∫£o h√†nh': 'warranty_policy',
                        'Tr·ªçng l∆∞·ª£ng': 'weight'
                    })

                # ===============================
                # 2) Ch·ªâ gi·ªØ ƒë√∫ng c√°c c·ªôt c·∫ßn thi·∫øt
                # KH√îNG CLEAN n·ªØa ƒë·ªÉ KH√îNG l·ªách pipeline nh·∫≠p tay
                # ===============================
                needed_cols = [
                    'brand', 'model', 'bike_type', 'origin', 'engine_capacity',
                    'registration_year', 'mileage_km', 'min_price', 'max_price'
                ]

                df = df_raw[needed_cols].copy()

                # ===============================
                # 3) Chuy·ªÉn NaN min/max v·ªÅ NaN (gi·ªëng nh·∫≠p tay)
                # ===============================
                df['min_price'] = df['min_price'].replace(0, np.nan)
                df['max_price'] = df['max_price'].replace(0, np.nan)

                # ===============================
                # 4) T√≠nh age gi·ªëng h·ªát nh·∫≠p tay
                # ===============================
                current_year = 2025
                df['age'] = current_year - pd.to_numeric(df['registration_year'], errors='coerce')

                # ===============================
                # 5) Apply grouping EXACT nh∆∞ nh·∫≠p tay
                # ===============================
                if helpers is not None:
                    # brand_grouped
                    df['brand_grouped'] = df['brand'].apply(
                        lambda b: 'H√£ng kh√°c' if b in helpers['rare_brands'] else b
                    )

                    # model_grouped theo t·ª´ng brand_grouped
                    def map_model(row):
                        bg = row['brand_grouped']
                        rare_models = helpers['model_group_maps'].get(bg, set())
                        return 'D√≤ng kh√°c' if row['model'] in rare_models else row['model']

                    df['model_grouped'] = df.apply(map_model, axis=1)

                    # segment
                    df['segment'] = df['brand_grouped'] + '_' + df['model_grouped']

                    # brand_meanprice
                    df['brand_meanprice'] = df['brand'].map(helpers['brand_mean_map'])

                else:
                    # fallback
                    df['brand_grouped'] = df['brand']
                    df['model_grouped'] = df['model']
                    df['segment'] = df['brand'] + '_' + df['model']
                    df['brand_meanprice'] = np.nan
                    st.warning("Kh√¥ng c√≥ helpers, d·ª± ƒëo√°n c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c.")

                # ===============================
                # 6) Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ predict
                # ===============================
                cat_cols = ['segment','bike_type','origin','engine_capacity']
                num_cols = ['age','mileage_km','min_price','max_price','brand_meanprice']

                X = df[cat_cols + num_cols]

                # ===============================
                # 7) Predict
                # ===============================
                df['predicted_price'] = np.expm1(model.predict(X))

                # ===============================
                # 8) Show result
                # ===============================
                st.write("K·∫øt qu·∫£ (10 d√≤ng ƒë·∫ßu):")
                st.dataframe(df.head(10))

                csv = df.to_csv(index=False).encode('utf-8')
                st.download_button("T·∫£i k·∫øt qu·∫£ (CSV)", data=csv, file_name="predictions.csv", mime='text/csv')

            except Exception as e:
                st.error(f"L·ªói x·ª≠ l√Ω file: {e}")


# --- ANOMALY PAGE ---
else:
    st.header("Ph√°t hi·ªán xe b·∫•t th∆∞·ªùng")

    # T·∫°o 2 TAB
    tab_user, tab_admin = st.tabs(["üë§ User ki·ªÉm tra xe", "üõ† Admin ki·ªÉm tra d·ªØ li·ªáu"])

    # ======================================
    # 1) TAB USER
    # ======================================
    with tab_user:

        # st.subheader("Nh·∫≠p tay 1 xe ƒë·ªÉ ki·ªÉm tra")

        # H√†m l∆∞u request user v√†o file Excel
        # def save_user_request(df_input):
        #     save_path = "user_submissions.xlsx"
        #     if os.path.exists(save_path):
        #         old = pd.read_excel(save_path)
        #         new = pd.concat([old, df_input], ignore_index=True)
        #     else:
        #         new = df_input.copy()

        #     new.to_excel(save_path, index=False)

        # H√†m l∆∞u request user v√†o file Excel
        def save_user_request(df_input):
            save_path = "user_submissions.xlsx"
            
            # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh thay ƒë·ªïi DataFrame g·ªëc (df_in)
            df_save = df_input.copy() 

            # 1. Ki·ªÉm tra xem c·ªôt 'post_time' c√≥ t·ªìn t·∫°i kh√¥ng
            if 'post_time' in df_save.columns:
                # 2. N·∫øu c·ªôt l√† timezone-aware (c√≥ m√∫i gi·ªù), chuy·ªÉn n√≥ th√†nh timezone-unaware
                if df_save['post_time'].dt.tz is not None:
                    # .dt.tz_localize(None) s·∫Ω lo·∫°i b·ªè th√¥ng tin m√∫i gi·ªù (GMT+7)
                    # D·ªØ li·ªáu ng√†y gi·ªù v·∫´n gi·ªØ nguy√™n gi√° tr·ªã theo gi·ªù ƒë·ªãa ph∆∞∆°ng (GMT+7)
                    df_save['post_time'] = df_save['post_time'].dt.tz_localize(None)

            if os.path.exists(save_path):
                old = pd.read_excel(save_path)
                new = pd.concat([old, df_save], ignore_index=True)
            else:
                new = df_save.copy()

            # ƒêo·∫°n n√†y s·∫Ω ch·∫°y tr∆°n tru v√¨ c·ªôt ng√†y gi·ªù ƒë√£ l√† timezone-unaware
            new.to_excel(save_path, index=False)

        # ============================
        # 1.1 Nh·∫≠p tay
        # ============================
        st.subheader("Nh·∫≠p th√¥ng tin xe c·∫ßn rao b√°n")
        col1, col2 = st.columns(2)

        with col1:
            brand = st.selectbox("Th∆∞∆°ng hi·ªáu", brand_list)
            model_name = st.selectbox("D√≤ng xe", model_list)
            bike_type = st.selectbox("Lo·∫°i xe", bike_type_list)
            origin = st.selectbox("Xu·∫•t x·ª©", origin_list)
            engine_capacity = st.selectbox("Dung t√≠ch", engine_capacity_list)

        with col2:
            registration_year = st.number_input("NƒÉm ƒëƒÉng k√Ω", 1980, 2025, 2019)
            mileage_km = st.number_input("S·ªë km ƒë√£ ƒëi", 0, value=10000)
            min_price = st.number_input("Kho·∫£ng gi√° min", 0)
            max_price = st.number_input("Kho·∫£ng gi√° max", 0)
            price = st.number_input("Gi√° ni√™m y·∫øt", 0, value=20000000)
        
        # Th√™m ng√†y gi·ªù ƒëƒÉng tin
        col_d, col_t = st.columns(2)

        # with col_d:
        #     post_date = st.date_input("Ng√†y ƒëƒÉng tin", value=pd.Timestamp.now().date())

        # with col_t:
        #     post_time = st.time_input("Gi·ªù ƒëƒÉng tin", value=pd.Timestamp.now().time())

        # # G·ªôp th√†nh datetime
        # post_datetime = pd.to_datetime(str(post_date) + " " + str(post_time))

        with col_d:
            # B·∫°n c√≥ th·ªÉ gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh l√† gi·ªù hi·ªán t·∫°i
            post_date = st.date_input("Ng√†y ƒëƒÉng tin", value=pd.Timestamp.now(tz=pytz.timezone('Asia/Ho_Chi_Minh')).date())

        with col_t:
            post_time = st.time_input("Gi·ªù ƒëƒÉng tin", value=pd.Timestamp.now(tz=pytz.timezone('Asia/Ho_Chi_Minh')).time())

        # G·ªôp th√†nh datetime v√† g√°n m√∫i gi·ªù:
        # 1. T·∫°o ƒë·ªëi t∆∞·ª£ng datetime th√¥ (naive datetime) t·ª´ date v√† time input
        naive_datetime = pd.to_datetime(str(post_date) + " " + str(post_time))

        # 2. ƒê·ªãnh nghƒ©a m√∫i gi·ªù Asia/Ho_Chi_Minh (GMT+7)
        vietnam_tz = pytz.timezone('Asia/Ho_Chi_Minh')

        # 3. G√°n m√∫i gi·ªù cho ƒë·ªëi t∆∞·ª£ng datetime
        post_datetime = vietnam_tz.localize(naive_datetime)

        # chu·∫©n b·ªã key cho session_state
        if "last_df_in" not in st.session_state:
            st.session_state["last_df_in"] = None
        if "last_anomaly" not in st.session_state:
            st.session_state["last_anomaly"] = None
        if "checked" not in st.session_state:
            st.session_state["checked"] = False

        if st.button("Ki·ªÉm tra"):
            df_in = pd.DataFrame([{
                "brand": brand,
                "model": model_name,
                "bike_type": bike_type,
                "origin": origin,
                "engine_capacity": engine_capacity,
                "registration_year": registration_year,
                "mileage_km": mileage_km,
                "min_price": min_price,
                "max_price": max_price,
                "price": price
            }])

            df_in["age"] = 2025 - df_in["registration_year"]
            df_in["post_time"] = post_datetime

            # Mapping using helpers
            if helpers is not None:
                if df_in.at[0, 'brand'] in helpers['rare_brands']:
                    df_in['brand_grouped'] = 'H√£ng kh√°c'
                else:
                    df_in['brand_grouped'] = df_in['brand']

                rare_models = helpers['model_group_maps'].get(df_in.at[0, 'brand_grouped'], set())
                if df_in.at[0, 'model'] in rare_models:
                    df_in['model_grouped'] = 'D√≤ng kh√°c'
                else:
                    df_in['model_grouped'] = df_in['model']

                df_in["segment"] = df_in["brand_grouped"] + "_" + df_in["model_grouped"]
                df_in["brand_meanprice"] = helpers["brand_mean_map"].get(df_in.at[0,"brand"], np.nan)
            else:
                df_in["brand_grouped"] = df_in["brand"]
                df_in["model_grouped"] = df_in["model"]
                df_in["segment"] = df_in["brand"] + "_" + df_in["model"]
                df_in["brand_meanprice"] = np.nan

            try:
                df_all, anomaly = detect_outliers(df_in, model_path=MODEL_PATH, input_is_df=True, helpers=helpers)

                # l∆∞u t·∫°m v√†o session ƒë·ªÉ d√πng sau khi user x√°c nh·∫≠n
                st.session_state["last_df_in"] = df_in
                st.session_state["last_anomaly"] = anomaly
                st.session_state["checked"] = True

            except Exception as e:
                st.exception(e)

        # N·∫øu ƒë√£ c√≥ k·∫øt qu·∫£ ki·ªÉm tra trong session_state th√¨ hi·ªÉn th·ªã
        if st.session_state.get("checked", False):
            df_in = st.session_state["last_df_in"]
            anomaly = st.session_state["last_anomaly"]

            if anomaly is None:
                st.info("Kh√¥ng c√≥ k·∫øt qu·∫£ ki·ªÉm tra.")
            else:
                if len(anomaly) > 0:
                    # x√°c ƒë·ªãnh reason d·ª±a tr√™n score nh∆∞ y√™u c·∫ßu (model/business)
                    # note: detect_outliers ƒë√£ t√≠nh score_model_based, score_business_based
                    r = []
                    if anomaly["score_model_based"].iloc[0] >= 50:
                        r.append("M√¥ h√¨nh c·∫£nh b√°o ph√°t hi·ªán")
                    if anomaly["flag_mileage_low"].iloc[0] == 1:
                        r.append("Logic nghi·ªáp v·ª• (S·ªë km ƒë√£ ƒëi th·∫•p b·∫•t th∆∞·ªùng)")
                    if anomaly["flag_mileage_high"].iloc[0] == 1:
                        r.append("Logic nghi·ªáp v·ª• (S·ªë km ƒë√£ ƒëi cao b·∫•t th∆∞·ªùng)")
                    reason_text = " + ".join(r) if r else "Kh√¥ng x√°c ƒë·ªãnh"

                    st.error(f"üö® Xe n√†y B·∫§T TH∆Ø·ªúNG ‚Äî do {reason_text}")
                    # st.dataframe(anomaly)

                    # h·ªèi user: c√≥ mu·ªën ƒëƒÉng kh√¥ng? + n√∫t x√°c nh·∫≠n l∆∞u
                    choice = st.radio("Xe n√†y b·∫•t th∆∞·ªùng, b·∫°n v·∫´n mu·ªën ƒëƒÉng tin kh√¥ng?", ["Kh√¥ng", "C√≥"], horizontal=True, key="confirm_post_radio")

                    if st.button("X√°c nh·∫≠n"):
                        if choice == "C√≥":
                            # chu·∫©n b·ªã b·∫£n l∆∞u: lo·∫°i b·ªè c·ªôt n·ªôi b·ªô tr∆∞·ªõc khi l∆∞u
                            # df_save = df_in.copy()
                            # cols_to_drop = ["brand_grouped", "model_grouped", "segment", "brand_meanprice"]
                            # df_save = df_save.drop(columns=[c for c in cols_to_drop if c in df_save.columns])
                            save_user_request(df_in) # save ƒë·ªß th√¥ng tin
                            st.success("ƒê√£ ƒëƒÉng tin.")
                            # reset flags
                            st.session_state["last_df_in"] = None
                            st.session_state["last_anomaly"] = None
                            st.session_state["checked"] = False
                        else:
                            st.info("B·∫°n ƒë√£ ch·ªçn kh√¥ng ƒëƒÉng tin n√†y.")
                            # reset session
                            st.session_state["last_df_in"] = None
                            st.session_state["last_anomaly"] = None
                            st.session_state["checked"] = False

                else:
                    st.success("Xe n√†y KH√îNG b·∫•t th∆∞·ªùng")
                    # Show n√∫t l∆∞u n·∫øu user mu·ªën (optional) ‚Äî t·ª± l∆∞u ho·∫∑c cho user b·∫•m
                    if st.button("ƒêƒÉng tin"):
                        # df_save = df_in.copy()
                        # cols_to_drop = ["brand_grouped", "model_grouped", "segment", "brand_meanprice"]
                        # df_save = df_save.drop(columns=[c for c in cols_to_drop if c in df_save.columns])
                        save_user_request(df_in)
                        st.success("ƒê√£ ƒëƒÉng tin.")
                        st.session_state["last_df_in"] = None
                        st.session_state["last_anomaly"] = None
                        st.session_state["checked"] = False




    # ======================================
    # 2) TAB ADMIN
    # ======================================
    with tab_admin:

        st.subheader("Ch·∫ø ƒë·ªô ki·ªÉm tra d√†nh cho Admin")

        mode_admin = st.radio(
            "Ch·ªçn c√°ch ki·ªÉm tra:",
            ["D·ªØ li·ªáu user nh·∫≠p h√¥m nay", "Upload file"],
            horizontal=True
        )

        save_path = "user_submissions.xlsx"

        # ============================================================
        # MODE 1: KI·ªÇM TRA D·ªÆ LI·ªÜU USER NH·∫¨P H√îM NAY
        # ============================================================
        if mode_admin == "D·ªØ li·ªáu user nh·∫≠p h√¥m nay":

            st.subheader("Danh s√°ch tin user ƒë√£ g·ª≠i")

            if os.path.exists(save_path):
                df_user = pd.read_excel(save_path)

                cols_to_hide = ["brand_grouped", "model_grouped", "segment", "brand_meanprice"]
                df_user_display = df_user.drop(columns=[c for c in cols_to_hide if c in df_user.columns])

                st.dataframe(df_user_display.sort_values(by='post_time', ascending=False))

                if st.button("Ch·∫°y ki·ªÉm tra b·∫•t th∆∞·ªùng (User submissions)"):
                    try:
                        df_all, anomaly = detect_outliers(
                            df_user,
                            model_path=MODEL_PATH,
                            input_is_df=True,
                            helpers=helpers
                        )

                        st.success(f"Ph√°t hi·ªán {len(anomaly)} b·∫•t th∆∞·ªùng")
                        anomaly_print = anomaly.copy()
                        cols_to_drop = ['brand_grouped', 'model_grouped', 'segment', 'brand_meanprice','price_hat','resid','resid_median','resid_std','resid_z','flag_resid','p10','p90'
]
                        anomaly_print = anomaly_print.drop(columns=[c for c in cols_to_drop if c in anomaly_print.columns])
                        st.dataframe(anomaly_print.sort_values(by='post_time', ascending=False).head(20))

                        # === B·∫ÆT ƒê·∫¶U TH√äM N√öT T·∫¢I XU·ªêNG ===
                        if len(anomaly) > 0:
                            # 1. T·∫°o t√™n file c√≥ ng√†y gi·ªù
                            now = datetime.now().strftime("%Y%m%d_%H%M%S")
                            file_name = f"anomaly_detection_user_{now}.csv"
                            
                            # 2. Chuy·ªÉn DataFrame sang CSV
                            # Lo·∫°i b·ªè m√∫i gi·ªù kh·ªèi c·ªôt 'post_time' tr∆∞·ªõc khi t·∫£i xu·ªëng n·∫øu c·∫ßn (ƒë·∫£m b·∫£o kh√¥ng l·ªói)
                            df_output = anomaly_print.copy()
                            if 'post_time' in df_output.columns and df_output['post_time'].dt.tz is not None:
                                df_output['post_time'] = df_output['post_time'].dt.tz_localize(None)

                            csv = df_output.to_csv(index=False).encode('utf-8')
                            
                            # 3. T·∫°o n√∫t t·∫£i xu·ªëng
                            st.download_button(
                                label="T·∫£i k·∫øt qu·∫£ b·∫•t th∆∞·ªùng (CSV)",
                                data=csv,
                                file_name=file_name,
                                mime='text/csv'
                            )
                        # === K·∫æT TH√öC TH√äM N√öT T·∫¢I XU·ªêNG ===

                    except Exception as e:
                        st.exception(e)

            else:
                st.info("‚ö† Ch∆∞a c√≥ user n√†o g·ª≠i d·ªØ li·ªáu.")


        # ============================================================
        # MODE 2: ADMIN UPLOAD FILE KI·ªÇM TRA
        # ============================================================
        else:
            st.subheader("Upload file ƒë·ªÉ Admin ki·ªÉm tra")

            file_admin = st.file_uploader(
                "Ch·ªçn file d·ªØ li·ªáu c·∫ßn ki·ªÉm tra (xlsx/csv)",
                type=["xlsx", "csv"],
                key="admin_upload_file"
            )

            if st.button("Ch·∫°y ki·ªÉm tra file Admin"):
                if file_admin is None:
                    st.error("Vui l√≤ng upload file tr∆∞·ªõc!")
                else:
                    with tempfile.NamedTemporaryFile(
                        delete=False,
                        suffix=os.path.splitext(file_admin.name)[1]
                    ) as tmp:
                        tmp.write(file_admin.getvalue())
                        excel_path = tmp.name

                    try:
                        df_in = preprocess_motobike_data(excel_path)
                        df_all, anomaly = detect_outliers(
                            df_in, 
                            model_path=MODEL_PATH, 
                            input_is_df=True, 
                            helpers=helpers
                        )

                        st.success(
                            f"Ho√†n t·∫•t ki·ªÉm tra. T·ªïng {len(df_in)} b·∫£n ghi ‚Äî ph√°t hi·ªán {len(anomaly)} b·∫•t th∆∞·ªùng."
                        )
                        # st.dataframe(anomaly.head(20))
                        anomaly_print = anomaly.copy()
                        cols_to_drop = ['brand_grouped', 'model_grouped', 'segment', 'brand_meanprice','price_hat','resid','resid_median','resid_std','resid_z','flag_resid','p10','p90'
]
                        anomaly_print = anomaly_print.drop(columns=[c for c in cols_to_drop if c in anomaly_print.columns])
                        st.dataframe(anomaly_print.head(20))

                        # === B·∫ÆT ƒê·∫¶U TH√äM N√öT T·∫¢I XU·ªêNG ===
                        if len(anomaly) > 0:
                            # 1. T·∫°o t√™n file c√≥ ng√†y gi·ªù
                            now = datetime.now().strftime("%Y%m%d_%H%M%S")
                            file_name = f"anomaly_detection_admin_{now}.csv"
                            
                            # 2. Chuy·ªÉn DataFrame sang CSV
                            df_output = anomaly_print.copy()
                            # N·∫øu c·ªôt post_time c√≥, h√£y lo·∫°i b·ªè m√∫i gi·ªù (ƒë·ªÉ tr√°nh l·ªói)
                            if 'post_time' in df_output.columns and df_output['post_time'].dt.tz is not None:
                                df_output['post_time'] = df_output['post_time'].dt.tz_localize(None)

                            csv = df_output.to_csv(index=False).encode('utf-8')
                            
                            # 3. T·∫°o n√∫t t·∫£i xu·ªëng
                            st.download_button(
                                label="T·∫£i k·∫øt qu·∫£ b·∫•t th∆∞·ªùng (CSV)",
                                data=csv,
                                file_name=file_name,
                                mime='text/csv'
                            )
                        # === K·∫æT TH√öC TH√äM N√öT T·∫¢I XU·ªêNG ===

                    except Exception as e:
                        st.exception(e)

st.sidebar.markdown("---")
st.sidebar.markdown("·ª®ng d·ª•ng cho ph√©p: 1) D·ª± ƒëo√°n gi√° xe m√°y 2) Ph√°t hi·ªán xe b·∫•t th∆∞·ªùng (nh·∫≠p tay ho·∫∑c upload file)")
